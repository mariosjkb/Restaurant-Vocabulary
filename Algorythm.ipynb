{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eρώτηση 4\n",
    "#### Μάριος Ιακωβίδης Α.Μ. 4063"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import sklearn.feature_extraction.text as sk_text\n",
    "import sklearn.cluster as sk_cluster\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a Yelp dataset that contains some restaurants in Philladelphia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(951, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "restaurants_df = pd.read_csv('philly_restaurants_categories.csv')\n",
    "display(restaurants_df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Yelp reviews for all businesses reviewed in Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6990280, 9)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load yelp reviews\n",
    "json_dir = \"C:/Users/mariosjkb/Desktop/3η Σειρα Ασκησεων/reviews_dataset\"\n",
    "\n",
    "json_pattern = os.path.join(json_dir,'*.json')\n",
    "file_list = glob.glob(json_pattern)\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for file in file_list:\n",
    "  json_data = pd.read_json(file, lines=True)\n",
    "  dfs.append(json_data)\n",
    "reviews_df = pd.concat(dfs,ignore_index=True)\n",
    "display(reviews_df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only the reviews for the restaurants we loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155680, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_df = pd.merge(reviews_df,restaurants_df,how=\"inner\",on=\"business_id\")\n",
    "data_df = data_df[['business_id','text','stars']]\n",
    "display(data_df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each restaurant join all the reviews and find the mean of the star rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(951, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_df = data_df.groupby(['business_id']).agg(({'text':' '.join,'stars':'mean'})).reset_index()\n",
    "data_df.to_csv('data.csv')\n",
    "display(data_df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label restaurants as good(represented as 1) if the mean star rating is above 4.5 and bad(represented as 0) if the rating is less that 2. We ignore the rest of the restaurants in order to keep the very best and the very worst of the them and get reviews with plenty of positive/negative comments. Also this helps to have well shaped and seperated clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164, 6)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "data = data.loc[(data['stars'] >= 4.5) | (data['stars'] <= 2)].reset_index()\n",
    "data['quality'] = np.where(data['stars'] >= 4.5,1,0)\n",
    "data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize data using td-idf vectorizer and ignore a list of stop words we created"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stop_words_list was created progressively after plenty of runs of the K-means algorythm. We checked the 10 most significant words in the cluster centers and if they weren't describing a restaurant in a positive or negative way, we considered them as stop-words in order to be ignored in the next run. After plenty of runs and after getting the result we wanted we stopped that process and moved forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164, 100)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_list = ['pizza', 'food', 'great', 'sushi', 'good', 'place', 'delicious', 'italian', 'best', 'just', 'amazing', 'like', 'service', 'order', 'time', 'pasta', 'really', 'ordered', 'fresh', 'definitely','food', 'order', 'drive', 'fries', 'service', 'time', 'place', 'just', 'like', 'location', 'burger', 'don', 'minutes', 'got', 'ordered', 'people', 've', 'good', 'chicken', 'wait','the', 'and', 'to', 'it', 'was', 'this', 'they', 'my', 'in', 'is', 'of', 'for', 'that', 'you', 'not', 'at', 'me', 'on', 'but', 'with','the', 'and', 'to', 'was', 'it', 'of', 'is', 'for', 'in', 'with', 'we', 'this', 'my', 'they', 'you', 'that', 'but', 'on', 'so', 'had','have', 'no', 'there', 'be', 'are', 'up', 'were', 'when', 'one', 'get', 'here', 'out', 'if', 'from', 'all', 'she', 'or', 'never', 'what', 'as','were', 'are', 'have', 'very', 'as', 'be', 'their', 'all', 'out', 'also', 'our', 'here', 'there', 'which', 'friendly', 'can', 'from', 'if', 'back', 'try','roll', 'go', 'will', 'philly', 'sauce', 'menu', 'by', 'cheese', 'about', 'been', 'would','go', 'even', 'an', 'your', 'do', 'he', 'them', 'because', 'after', 'only', 'then', 'been', 'will', 'would', 'her', 'about', 'said', 'give', 'ever', 'again','through', 'didn', 'always', 'over', 'other', 'know', 'went', 'who', 'has', 'sandwich', 'come', 'down', 'how', '10', 'staff', 'could', 'two', 'long', 'some', 'more','rolls','us', 'little', 'restaurant', 'everything', 'too', 'made', 'spicy','should', 'still', 'times', 'any', 'around', 'did', 'want', 'took', 'right', 'came', 'before', 'another', 'something', 'us', 'meal', 'than', 'night', 're','mcdonald', 'customer', 'told', 'asked', 'every', 'first', 'while', 'off', 'these', 'going', 'say', 'better', 'eat', 'hot', 'take', 'day', 'most', 'see', 'make','hour', 'way', 'gave', 'inside', 'wasn', 'away', 'well', 'now', 'much', 'll', 'his', 'am', 'experience', 'being', 'last', 'new', 'think', 'big', 'need', 'wanted', 'stars', 'its', 'sure', 'pretty', 'home', 'nice', 'into', 'area', 'thing', 'though', 'tried', 'since', 'taste', 'point', 'why', 'ice', 'open', 'where', 'large', 'next', 'each', 'lot',  'many', 'find', 'things', 'check', 'items', 'lunch', 'dining','byob', 'pork', 'lunch',]\n",
    "vectorizer = sk_text.TfidfVectorizer(stop_words=stop_words_list,max_features=100)\n",
    "reviews = data.loc[:,\"text\"].to_list()\n",
    "clustering_data = vectorizer.fit_transform(reviews)\n",
    "clustering_data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a K-means clustering algorythm using the vectorized data into 2 clusters(good restaurants and bad restaurants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = sk_cluster.KMeans(n_clusters=2,init=\"k-means++\",n_init=50)\n",
    "kmeans_result = kmeans.fit_transform(clustering_data)\n",
    "kmeans_labels = kmeans.labels_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to help with the mapping of true labels and K-means labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_class_mapping(kmeans_labels,true_labels):\n",
    "    C= metrics.confusion_matrix(kmeans_labels,true_labels)\n",
    "    mapping = list(np.argmax(C,axis=1)) #for each row (cluster) find the best class in the confusion matrix\n",
    "    mapped_kmeans_labels = [mapping[l] for l in kmeans_labels]\n",
    "    C2= metrics.confusion_matrix(mapped_kmeans_labels,true_labels)\n",
    "    return mapped_kmeans_labels,C2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics to evaluate the quality of the clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[87  0]\n",
      " [ 5 72]]\n",
      "Clustering accuracy =  0.9695121951219512\n",
      "Precision score per class =  [0.94565217 1.        ]\n",
      "Recall score per class =  [1.         0.93506494]\n",
      "F1-score per class =  [0.97206704 0.96644295]\n"
     ]
    }
   ],
   "source": [
    "labels, C = cluster_class_mapping(kmeans_labels,data.quality)\n",
    "print(\"Confusion matrix:\\n\",C)\n",
    "\n",
    "accuracy = metrics.accuracy_score(labels,data.quality)\n",
    "print(\"Clustering accuracy = \",accuracy)\n",
    "\n",
    "precision = metrics.precision_score(labels,data.quality,average=None)\n",
    "print(\"Precision score per class = \",precision)\n",
    "\n",
    "recall = metrics.recall_score(labels,data.quality,average=None)\n",
    "print(\"Recall score per class = \",recall)\n",
    "\n",
    "f1_score = metrics.f1_score(labels,data.quality,average=None)\n",
    "print(\"F1-score per class = \",f1_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude that the result of the clustering is almost 100% accurate, since all the metrics are above 93% and the confusion matrix is almost ideal. So our goal to create well seperated clusters was met."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the 10 most important words in the cluster centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for cluster 0 are: ['thru', 'worst', 'manager', 'fast', 'cold', 'waiting', 'bad', 'delivery', 'wrong', 'work']\n",
      "Top 10 words for cluster 1 are: ['spot', 'love', 'recommend', 'favorite', 'salad', 'excellent', 'dinner', 'perfect', 'special', 'small']\n"
     ]
    }
   ],
   "source": [
    "cluster_centers = kmeans.cluster_centers_\n",
    "cluster_centers_word_indices = (-cluster_centers).argsort()\n",
    "top_10_word_indices = []\n",
    "top_10_words_all = []\n",
    "\n",
    "for i in range(0,cluster_centers_word_indices.shape[0]):\n",
    "    top_10_word_indices.append(cluster_centers_word_indices[i][:10])\n",
    "\n",
    "words = vectorizer.get_feature_names_out()\n",
    "\n",
    "for i in range(0,len(top_10_word_indices)):\n",
    "    top_10_words = []\n",
    "    for j in range(0,10):\n",
    "        top_10_words.append(words[top_10_word_indices[i][j]])\n",
    "\n",
    "    print(\"Top 10 words for cluster \" + str(i) + \" are: \" + str(top_10_words))\n",
    "    top_10_words_all.append(top_10_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data in train and test set in order to use a Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.text\n",
    "y = data.quality\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Skipgram embedding to get the 5 more frequent context words of the positive and negative words given by the cluster centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thru drive', 'thru line', 'thru speaker', 'thru through', 'thru cars', 'worst slowest', 'worst mcdonalds', 'worst poor', 'worst mcdonald', 'worst worse', 'manager spoke', 'manager phone', 'manager refused', 'manager situation', 'manager male', 'fast attitudes', 'fast mediocre', 'fast atrocious', 'fast dominos', 'fast horrendous', 'cold lukewarm', 'cold soggy', 'cold stale', 'cold french', 'cold nuggets', 'waiting fifteen', 'waiting eventually', 'waiting twenty', 'waiting mins', 'waiting min', 'bad compelled', 'bad dominos', 'bad poorly', 'bad warn', 'bad poisoning', 'delivery grubhub', 'delivery online', 'delivery delivered', 'delivery estimated', 'delivery via', 'wrong correct', 'wrong label', 'wrong repeat', 'wrong didnt', 'wrong turns', 'work showing', 'work purpose', 'work running', 'work wonder', 'work wants', 'spot kinme', 'spot rotation', 'spot nolibs', 'spot jewelers', 'spot fairmount', 'love suppli', 'love lucatelli', 'love officially', 'love treats', 'love awesome', 'recommend highly', 'recommend suggest', 'recommend recommended', 'recommend lovers', 'recommend anyone', 'favorite fav', 'favorite fave', 'favorite favorites', 'favorite faves', 'favorite simi', 'salad kani', 'salad seaweed', 'salad beets', 'salad wontons', 'salad caesar', 'excellent superb', 'excellent terrific', 'excellent outstanding', 'excellent phenomenal', 'excellent exceptional', 'dinner valentine', 'dinner birthday', 'dinner fiance', 'dinner anniversary', 'dinner ptg', 'perfect compliment', 'perfect light', 'perfect ending', 'perfect vodka', 'perfect decadent', 'special speciality', 'special chose', 'special bluefin', 'special cobb', 'special artichoke', 'small fits', 'small homey', 'small downside', 'small groups', 'small cramped']\n"
     ]
    }
   ],
   "source": [
    "top_phrases = []\n",
    "# preprocess train data\n",
    "train_gsim = [gensim.utils.simple_preprocess(x) for x in X_train]\n",
    "train_data_labels = [(x,y) for (x,y) in zip(train_gsim,y_train) if len(x) > 0]\n",
    "X_train_gsim = [x for (x,y) in train_data_labels]\n",
    "y_train_gsim = [y for (x,y) in train_data_labels]\n",
    "\n",
    "# preprocess test data\n",
    "test_gsim = [gensim.utils.simple_preprocess(x) for x in X_test]\n",
    "test_data_labels = [(x,y) for (x,y) in zip(test_gsim,y_test) if len(x) > 0]\n",
    "X_test_gsim = [x for (x,y) in test_data_labels]\n",
    "y_test_gsim = [y for (x,y) in test_data_labels]\n",
    "\n",
    "# train a Skipgram model and get the new train set and test set\n",
    "skipgram_model = gensim.models.Word2Vec(X_train_gsim,min_count=1,vector_size=50,window=10,sg=1)\n",
    "\n",
    "# for each word find the 5 most frequent context words\n",
    "for i in range(0,2):\n",
    "    for j in range(0,len(top_10_words_all[i])):\n",
    "        for k in range(0,5):\n",
    "            phrase = top_10_words_all[i][j] + \" \" + skipgram_model.wv.most_similar(top_10_words_all[i][j])[k][0]\n",
    "            top_phrases.append(phrase)\n",
    "\n",
    "print(top_phrases)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the top phrases list we evaluated which words or phrases are describing a restaurant in a positive or a negative way and we created the resprective vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_vocabulary = ['spot lovers','love','awesome','highly recommend','suggest','strongly recommend','favorite','excellent','outstanding','phenomenal','superb',\n",
    "                       'valentine dinner','fiance dinner','celebration dinner','perfect','decadent','special','suggest','become favorite','outstanding','fantastic',\n",
    "                       'perfect light']\n",
    "\n",
    "negative_vocabulary = ['drive thru','worst','worse','slowest','manager refused','spoke to manager','fast','atrocious','mediocre','cold food','stale food','soggy food','waiting twenty mins',\n",
    "                       'bad','terrible','awful','wrong everytime','garbage','horrible','bad poisoning','wrong label','horrendous']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive and negative restaurant vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive vocabulary for restaurants:  ['spot lovers', 'love', 'awesome', 'highly recommend', 'suggest', 'strongly recommend', 'favorite', 'excellent', 'outstanding', 'phenomenal', 'superb', 'valentine dinner', 'fiance dinner', 'celebration dinner', 'perfect', 'decadent', 'special', 'suggest', 'become favorite', 'outstanding', 'fantastic', 'perfect light']\n",
      "\n",
      "\n",
      "Negative vocabulary for restaurants:  ['drive thru', 'worst', 'worse', 'slowest', 'manager refused', 'spoke to manager', 'fast', 'atrocious', 'mediocre', 'cold food', 'stale food', 'soggy food', 'waiting twenty mins', 'bad', 'terrible', 'awful', 'wrong everytime', 'garbage', 'horrible', 'bad poisoning', 'wrong label', 'horrendous']\n"
     ]
    }
   ],
   "source": [
    "print(\"Positive vocabulary for restaurants: \",positive_vocabulary)\n",
    "print(\"\\n\")\n",
    "print(\"Negative vocabulary for restaurants: \",negative_vocabulary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5cbc1a155237b9bce617c4862746742149ea111f2110e16ccccf60216224e135"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
